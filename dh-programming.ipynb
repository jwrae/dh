{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python for Research: Examples\n",
    "\n",
    "David J. Thomas, [thePortus.com](http://theportus.com), created Spring 2018\n",
    "\n",
    "Various examples of short applied uses of Python for the students of Hacking History at the University of South Florida.\n",
    "\n",
    "To find many more examples, check out [The Programming Historian](programminghistorian.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Documentation\n",
    "\n",
    "To help you find help using key packages, here are links and documentation relevant to general/important Python packages for working with data. In addition, below you will find documentation links relevant to each specific example.\n",
    "\n",
    "* [Current Python3 Documentation](https://docs.python.org/3/) - Central spot for all official python documentation\n",
    "* [Built-in Package - os](https://docs.python.org/3/library/os.html) - Critical package for working with files, getting/building system paths\n",
    "* [Built-in Packages - csv](https://docs.python.org/3/library/csv.html) - Critical package for working with csv data, can read csvs intelligently\n",
    "* [Pip package website](https://pypi.python.org/pypi) - Search for additional Python packages to add with `pip install package_name`\n",
    "\n",
    "*Note*: depending upon your Python installation, you may need to type `pip3` instead of `pip` in your terminal to install these packages correctly.\n",
    "\n",
    "*Note*: you need to run the examples in order, as many cells depend on those above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE 1: Scraping Historical Data\n",
    "\n",
    "Uses the 'requests' and 'BeautifulSoup' packages to request data from [Florida Memory's website](https://www.floridamemory.com/) of a church survey conducted by the WPA (Works Progress Administration) in the 1930's.\n",
    "\n",
    "``` sh\n",
    "pip install requests\n",
    "pip install BeautifulSoup4\n",
    "```\n",
    "\n",
    "* [Requests Documentation](http://docs.python-requests.org/en/master/)\n",
    "* [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [BeautifulSoup Guide](http://web.stanford.edu/~zlotnick/TextAsData/Web_Scraping_with_Beautiful_Soup.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "Antioch Baptist Church https://www.floridamemory.com/items/show/246670\n",
      "Date: 1917\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Newberry\n",
      "=========================\n",
      "Baptist Church of Antioch https://www.floridamemory.com/items/show/246671\n",
      "Date: 1872\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: La Crosse\n",
      "=========================\n",
      "Corinth Baptist Church https://www.floridamemory.com/items/show/246672\n",
      "Date: 1879\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Newberry\n",
      "=========================\n",
      "Damack Baptist Church https://www.floridamemory.com/items/show/246673\n",
      "Date: 1902\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Campville\n",
      "=========================\n",
      "Eden Baptist Church https://www.floridamemory.com/items/show/246674\n",
      "Date: 1902\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Rex Community\n",
      "=========================\n",
      "Eliam Baptist Church https://www.floridamemory.com/items/show/246675\n",
      "Date: 1859\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Melrose\n",
      "=========================\n",
      "Emanuel Missionary Baptist Church https://www.floridamemory.com/items/show/246676\n",
      "Date: 1932\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Gainesville\n",
      "=========================\n",
      "Fairbanks Baptist Church https://www.floridamemory.com/items/show/246677\n",
      "Date: 1932\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Fairbanks\n",
      "=========================\n",
      "First Baptist Church https://www.floridamemory.com/items/show/246678\n",
      "Date: 1886\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: High Springs\n",
      "=========================\n",
      "First Baptist Church of Alachua https://www.floridamemory.com/items/show/246679\n",
      "Date: 1898\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Alachua\n",
      "=========================\n",
      "First Baptist Church of Archer https://www.floridamemory.com/items/show/246680\n",
      "Date: 1920\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Archer\n",
      "=========================\n",
      "First Baptist Church of Campville https://www.floridamemory.com/items/show/246681\n",
      "Date: 1892\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Campville\n",
      "=========================\n",
      "First Baptist Church of Gainesville https://www.floridamemory.com/items/show/246682\n",
      "Date: 1870\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Gainesville\n",
      "=========================\n",
      "First Baptist Church of Hawthorne https://www.floridamemory.com/items/show/246683\n",
      "Date: 1852\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Hawthorne\n",
      "=========================\n",
      "First Baptist Church of Island Grove https://www.floridamemory.com/items/show/246684\n",
      "Date: 1902\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Island Grove\n",
      "=========================\n",
      "First Baptist Church of Newberry https://www.floridamemory.com/items/show/246685\n",
      "Date: 1896\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Newberry\n",
      "=========================\n",
      "First Baptist Church of Orange Heights https://www.floridamemory.com/items/show/246686\n",
      "Date: 1912\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Orange Heights\n",
      "=========================\n",
      "First Baptist Church of Waldo https://www.floridamemory.com/items/show/246687\n",
      "Date: 1882\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Waldo\n",
      "=========================\n",
      "First Morning Star Baptist Church https://www.floridamemory.com/items/show/246688\n",
      "Date: 1875\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: African-American\n",
      "Denomination: Baptist\n",
      "City: Gainesville\n",
      "=========================\n",
      "Forest Grove Baptist Church https://www.floridamemory.com/items/show/246689\n",
      "Date: 1888\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Forest Grove\n",
      "=========================\n",
      "Fort Clark Baptist Church https://www.floridamemory.com/items/show/246690\n",
      "Date: 1850\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Gainesville\n",
      "=========================\n",
      "Fort Clark Baptist Church https://www.floridamemory.com/items/show/246691\n",
      "Date: 1867\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: African-American\n",
      "Denomination: Baptist\n",
      "City: Gainesville\n",
      "=========================\n",
      "Friendship Baptist Church https://www.floridamemory.com/items/show/246692\n",
      "Date: 1888\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Gainesville\n",
      "=========================\n",
      "Galilee Baptist Church of Hawthorne https://www.floridamemory.com/items/show/246693\n",
      "Date: 1885\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Hawthorne\n",
      "=========================\n",
      "Grover Baptist Church https://www.floridamemory.com/items/show/246694\n",
      "Date: 1898\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Alachua\n",
      "=========================\n",
      "Hague Baptist Church https://www.floridamemory.com/items/show/246695\n",
      "Date: 1911\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Hague\n",
      "=========================\n",
      "Hopewell Baptist Church https://www.floridamemory.com/items/show/246696\n",
      "Date: 1912\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Micanopy\n",
      "=========================\n",
      "Johnson Chapel Baptist Church https://www.floridamemory.com/items/show/246697\n",
      "Date: 1917\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Gainesville\n",
      "=========================\n",
      "Jonesville Baptist Church https://www.floridamemory.com/items/show/246698\n",
      "Date: 1860\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: Newberry\n",
      "=========================\n",
      "LaCrosse Baptist Church https://www.floridamemory.com/items/show/246699\n",
      "Date: 1882\n",
      "Collection: WPA Church Records\n",
      "Race/Ethnicity: White\n",
      "Denomination: Baptist\n",
      "City: LaCrosse\n"
     ]
    }
   ],
   "source": [
    "# importing dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\"\"\" soup_url\n",
    "Receives a url, makes an http request, then parses the response into a BeautifulSoup object.\n",
    "\n",
    "@param    {string}           url       page to convert\n",
    "@return   {BeautifulSoup}              object created with page html\n",
    "\"\"\"\n",
    "def soup_url(url):\n",
    "    # uses requests.get to fetch data object\n",
    "    page_response = requests.get(url)\n",
    "    # grab page html from the string in .text\n",
    "    page_html = page_response.text\n",
    "    # parse the raw html string into a smart Python object with BeautifulSoup\n",
    "    page_soup = BeautifulSoup(page_html, 'html.parser')\n",
    "    # return new 'souped' page object\n",
    "    return page_soup\n",
    "\n",
    "\n",
    "\"\"\" Main Script Start \"\"\"\n",
    "\n",
    "# url for the first page of results\n",
    "start_url = 'https://www.floridamemory.com/solr-search/results/?q=*:*%5E10%20AND%20collection%3A%22WPA%20Church%20Records%22&compact=0&query=&searchbox=11&county=&ethnicity=&year='\n",
    "# make web request and convert into smart object\n",
    "start_page = soup_url(start_url)\n",
    "# store list of elements with result data\n",
    "results = start_page.find_all('div', class_='search-item')\n",
    "\n",
    "# loop through each result get each data point\n",
    "for result in results:\n",
    "    # gets the 'a' tag surrounding the header\n",
    "    item_header = result.find('h3').find('a')\n",
    "    # get the name displayed on the page\n",
    "    item_name = item_header.get_text()\n",
    "    # get the link stored in the 'a' tag's 'href' property\n",
    "    item_link = item_header['href']\n",
    "    # get each of the metadata 'columns' as a list\n",
    "    item_info = result.find_all('dd')\n",
    "    print('=========================')\n",
    "    # print the name of the church and the link to it's full page\n",
    "    print(item_name, item_link)\n",
    "    # print each of the metadata columns, with extra whitespace remoted\n",
    "    print('Date:', item_info[0].get_text().strip())\n",
    "    print('Collection:', item_info[1].get_text().strip())\n",
    "    print('Race/Ethnicity:', item_info[2].get_text().strip())\n",
    "    print('Denomination:', item_info[3].get_text().strip())\n",
    "    print('City:', item_info[4].get_text().strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE 2: Getting Wikipedia Data\n",
    "\n",
    "Uses the official python package released by Wikipedia that makes use of Wikipedia's API ([what is this?](https://en.wikipedia.org/wiki/Application_programming_interface)). Can quickly gather data from wikipedia in a python friendly format\n",
    "\n",
    "* [Wikipedia package documentation](https://pypi.python.org/pypi/wikipedia/) - pip pack with examples of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting wikipage: Digital humanities\n",
      "Writing to output/Digital humanities.txt\n",
      "Getting wikipage: Feminist digital humanities\n",
      "Writing to output/Feminist digital humanities.txt\n",
      "Getting wikipage: Humanities\n",
      "Writing to output/Humanities.txt\n",
      "Getting wikipage: Alliance of Digital Humanities Organizations\n",
      "Writing to output/Alliance of Digital Humanities Organizations.txt\n",
      "Getting wikipage: Digital Humanities conference\n",
      "Writing to output/Digital Humanities conference.txt\n",
      "Getting wikipage: Digital Humanities Observatory\n",
      "Writing to output/Digital Humanities Observatory.txt\n",
      "Getting wikipage: European Association for Digital Humanities\n",
      "Writing to output/European Association for Digital Humanities.txt\n",
      "Getting wikipage: Canadian Society for Digital Humanities\n",
      "Writing to output/Canadian Society for Digital Humanities.txt\n",
      "Getting wikipage: Digital Humanities Quarterly\n",
      "Writing to output/Digital Humanities Quarterly.txt\n",
      "Getting wikipage: Digital Scholarship in the Humanities\n",
      "Writing to output/Digital Scholarship in the Humanities.txt\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "\"\"\" save_wikipage\n",
    "Gets the name of a wikipedia page, fetches its data. Then save the content to a .txt file matching the name of the page\n",
    "\n",
    "@param    {string}           page_name     title of wikipage\n",
    "@return   {boolean}                        true if successful\n",
    "\"\"\"\n",
    "def save_wikipage(page_name):\n",
    "    print('Getting wikipage:', page_name)\n",
    "    wikipage = wikipedia.page(page_name)\n",
    "    content = wikipage.content\n",
    "    # save into the output folder a txt file with the name of the page title\n",
    "    filename = 'output/' + wikipage.title + '.txt'\n",
    "    print('Writing to', filename)\n",
    "    # open (new) file in write mode and write content to file\n",
    "    with open(filename, 'w+') as output_file:\n",
    "        output_file.write(content)\n",
    "    return True\n",
    "\n",
    "\n",
    "\"\"\" Main Script Start \"\"\"\n",
    "\n",
    "# get list of titles matching our search\n",
    "result_names = wikipedia.search('Digital Humanities')\n",
    "\n",
    "# loop through list of page names\n",
    "for result_name in result_names:\n",
    "    save_wikipage(result_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE 3: Parsing Linguistic Data\n",
    "\n",
    "Uses the famous Natural Language Toolkit to automatically handle language analysis in most major modern languages\n",
    "\n",
    "* [NLTK Documentation](http://www.nltk.org/) - Official Documentation from NLTK.org\n",
    "\n",
    "**NOTE**: Does not work on Windows machines!\n",
    "\n",
    "``` sh\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "Then, before you use nltk for the first time, launch python by typing `python` or `python3` in the terminal, then type the following\n",
    "\n",
    "``` python\n",
    "import nltk\n",
    "nltk.download()\n",
    "```\n",
    "\n",
    "If you are on a mac, a graphical interface will pop up allowing you to download packages. If you are on linux, you will have to use a command line interface. Again, this **will not work on Windows**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of tweets\n",
      "---\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n",
      "@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n",
      "@97sides CONGRATS :)\n",
      "yeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n",
      "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n",
      "#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n",
      "We don't like to keep our lovely customers waiting for long! We hope you enjoy! Happy Friday! - LWWF :) https://t.co/smyYriipxI\n",
      "@Impatientraider On second thought, there’s just not enough time for a DD :) But new shorts entering system. Sheep must be buying.\n",
      "Jgh , but we have to go to Bayan :D bye\n",
      "---\n",
      "Stats:\n",
      "Total number of adjectives =  6094\n",
      "Total number of nouns =  13180\n",
      "---\n",
      "Sample Tweet: skype was fun :-)\n",
      "---\n",
      "Sample Tokens: ['skype', 'was', 'fun', ':-)']\n",
      "---\n",
      "Sample PoS Tags: [('skype', 'NN'), ('was', 'VBD'), ('fun', 'JJ'), (':-)', 'JJ')]\n",
      "---\n",
      "Sample Chunks: (S skype/NN was/VBD fun/JJ :-)/JJ)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag_sents\n",
    "\n",
    "# get sample data as series of strings\n",
    "tweets = twitter_samples.strings('positive_tweets.json')\n",
    "# get same data as series of lists of tokens\n",
    "tweets_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "# batch part of speach tag every tweet\n",
    "tweets_tagged = pos_tag_sents(tweets_tokens)\n",
    "\n",
    "# chose a sample tweet to analyze\n",
    "sample_tweet_number = 250\n",
    "sample_tweet = tweets[sample_tweet_number]\n",
    "sample_tweets_token = tweets_tokens[sample_tweet_number]\n",
    "sample_tweets_tags = tweets_tagged[sample_tweet_number]\n",
    "sample_tweets_chunked = nltk.chunk.ne_chunk(sample_tweets_tags)\n",
    "\n",
    "# count adjectives and nouns in tweets by looping\n",
    "JJ_count = 0\n",
    "NN_count = 0\n",
    "for tweet in tweets_tagged:\n",
    "    for pair in tweet:\n",
    "        tag = pair[1]\n",
    "        if tag == 'JJ':\n",
    "            JJ_count += 1\n",
    "        elif tag == 'NN':\n",
    "            NN_count += 1\n",
    "\n",
    "# preview the tweets\n",
    "print('Sample of tweets')\n",
    "print('---')\n",
    "for tweet in tweets[0:9]:\n",
    "    print(tweet)\n",
    "\n",
    "\n",
    "print('---')\n",
    "print('Stats:')\n",
    "print('Total number of adjectives = ', JJ_count)\n",
    "print('Total number of nouns = ', NN_count)\n",
    "print('---')\n",
    "print('Sample Tweet:', sample_tweet)\n",
    "print('---')\n",
    "print('Sample Tokens:', sample_tweets_token)\n",
    "print('---')\n",
    "print('Sample PoS Tags:', sample_tweets_tags)\n",
    "print('---')\n",
    "print('Sample Chunks:', sample_tweets_chunked)\n",
    "\n",
    "# draw a sample sentence bank\n",
    "t = nltk.corpus.treebank.parsed_sents('wsj_0001.mrg')[0].draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# To see a list of all possible nltk PoS tags, run this cell\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE 4: Parsing Classical Linguistic Data\n",
    "\n",
    "Uses the new Classical Language Toolkit to automatically handle ancient / obsure language analysis (gaining more languages all the time)\n",
    "\n",
    "* [CLTK Documentation](cltk.readthedocs.org) - Official CLTK Documentation\n",
    "\n",
    "**NOTE**: Does not work on Windows machines!\n",
    "\n",
    "``` sh\n",
    "pip install cltk\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading greek_software_tlgu\n",
      "Downloading greek_text_perseus\n",
      "Downloading phi7143.79 MiB | 6.14 MiB/s \n",
      "Downloading tlg\n",
      "Downloading greek_proper_names_cltk\n",
      "Downloading greek_models_cltk\n",
      "Downloading greek_treebank_perseus\n",
      "Downloading greek_lexica_perseus\n",
      "Downloading greek_training_set_sentence_cltk\n",
      "Downloading greek_word2vec_cltk\n",
      "Downloading greek_text_lacus_curtius\n",
      "Downloading greek_text_first1kgreeks \n",
      "Downloading latin_text_perseus.79 MiB/s \n",
      "Downloading latin_treebank_perseusMiB/s \n",
      "Downloading latin_text_latin_library\n",
      "Downloading phi535.50 MiB | 6.79 MiB/s \n",
      "Downloading phi7\n",
      "Downloading latin_proper_names_cltk\n",
      "Downloading latin_models_cltk\n",
      "Downloading latin_pos_lemmata_cltk\n",
      "Downloading latin_treebank_index_thomisticus\n",
      "Downloading latin_lexica_perseus\n",
      "Downloading latin_training_set_sentence_cltk\n",
      "Downloading latin_word2vec_cltk\n",
      "Downloading latin_text_antique_digiliblt\n",
      "Downloading latin_text_corpus_grammaticorum_latinorum\n",
      "Downloading latin_text_poeti_ditalia \n"
     ]
    }
   ],
   "source": [
    "# This entire cell will automatically download any required corpora for greek or latin linguistic analysis.\n",
    "# It does not do anything else\n",
    "# It may take some time\n",
    "\n",
    "import cltk\n",
    "\n",
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "\n",
    "corpus_importer = CorpusImporter('greek')\n",
    "\n",
    "for required_corpus in corpus_importer.list_corpora:\n",
    "    try:\n",
    "        print('Downloading', required_corpus)\n",
    "        corpus_importer.import_corpus(required_corpus)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "corpus_importer = CorpusImporter('latin')\n",
    "\n",
    "for required_corpus in corpus_importer.list_corpora:\n",
    "    try:\n",
    "        print('Downloading', required_corpus)\n",
    "        corpus_importer.import_corpus(required_corpus)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Passage [2]\n",
      "        Hoc idem fit in reliquis civitatibus: in omnibus partibus incendia conspiciuntur; quae etsi magno cum dolore omnes ferebant, tamen hoc sibi solati proponebant, quod se prope explorata victoria celeriter amissa reciperaturos confidebant.\n",
      "---\n",
      "Tagged Passage [('[', 'U--------'), ('2', None), (']', 'U--------'), ('hoc', 'P-S---NA-'), ('idem', 'P-S---NA-'), ('fit', 'V3SPIA---'), ('in', 'R--------'), ('reliquis', 'A-P---NB-'), ('civitatibus', 'N-P---FB-'), (':', None), ('in', 'R--------'), ('omnibus', 'A-P---MB-'), ('partibus', 'N-P---MB-'), ('incendia', 'N-P---NA-'), ('conspiciuntur', None), (';', None), ('quae', 'P-S---FN-'), ('etsi', 'C--------'), ('magno', 'A-S---NB-'), ('cum', 'R--------'), ('dolore', 'N-S---MB-'), ('omnes', 'A-P---MN-'), ('ferebant', 'V3PIIA---'), (',', 'U--------'), ('tamen', 'D--------'), ('hoc', 'P-S---MB-'), ('sibi', 'P-S---MD-'), ('solati', None), ('proponebant', None), (',', 'U--------'), ('quod', 'C--------'), ('se', 'P-S---MA-'), ('prope', 'D--------'), ('explorata', 'T-PRPPNN-'), ('victoria', 'N-S---FB-'), ('celeriter', 'D--------'), ('amissa', 'T-SRPPFB-'), ('reciperaturos', None), ('confidebant', None), ('.', 'U--------')]\n",
      "---\n",
      "Lemmatized Passage ['[', '2', ']', 'hic', 'idem', 'fio', 'in', 'reliquus', 'civitas', ':', 'in', 'omne', 'pars', 'incendium', 'conspicio1', ';', 'qui1', 'etsi', 'magnus', 'cum', 'dolor', 'omnes', 'fero', ',', 'tamen', 'hic', 'sui', 'solati', 'propono', ',', 'qui1', 'sui', 'prope', 'exploro', 'victoria', 'celeriter', 'amitto', 'reciperaturos', 'confido.']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "from cltk.tag.pos import POSTag\n",
    "from cltk.stem.lemma import LemmaReplacer\n",
    "\n",
    "# make cltk objects configured for latin\n",
    "tagger = POSTag('latin')\n",
    "lemmatizer = LemmaReplacer('latin')\n",
    "\n",
    "# create a placeholder for file data\n",
    "passages = []\n",
    "\n",
    "with open('sample_data/caesar_gallic_wars.csv', 'r+') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    for row in reader:\n",
    "        passages.append(row)\n",
    "\n",
    "sample_passage_num = 500\n",
    "sample_passage = passages[sample_passage_num]['text']\n",
    "sample_passage_tagged = tagger.tag_ngram_123_backoff(sample_passage.lower())\n",
    "sample_passage_lemmatized = lemmatizer.lemmatize(sample_passage.lower())\n",
    "\n",
    "print('Sample Passage', sample_passage)\n",
    "print('---')\n",
    "print('Tagged Passage', sample_passage_tagged)\n",
    "print('---')\n",
    "print('Lemmatized Passage', sample_passage_lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.generate import generate, demo_grammar\n",
    "from nltk import CFG\n",
    "grammar = CFG.fromstring(demo_grammar)\n",
    "print(grammar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
